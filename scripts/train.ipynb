{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model import LayerNormNet\n",
    "from data_utils import transfer_dataset\n",
    "from losses import criterion\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter  # 引入 TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binyun/Project/SimProteinData/scripts/data_utils.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embed_list = [torch.load(os.path.join(embed_path,embed)).cpu().numpy() for embed in embeds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], spearman: -0.0031, Loss: 204.9690\n",
      "Epoch [2/1000], spearman: -0.0017, Loss: 204.9641\n",
      "Epoch [3/1000], spearman: 0.0039, Loss: 204.9639\n",
      "Epoch [4/1000], spearman: 0.0028, Loss: 204.9640\n",
      "Epoch [5/1000], spearman: -0.0054, Loss: 204.9644\n",
      "Epoch [6/1000], spearman: -0.0054, Loss: 204.9645\n",
      "Epoch [7/1000], spearman: -0.0024, Loss: 204.9641\n",
      "Epoch [8/1000], spearman: -0.0081, Loss: 204.9640\n",
      "Epoch [9/1000], spearman: 0.0084, Loss: 204.9633\n",
      "Epoch [10/1000], spearman: -0.0003, Loss: 204.9640\n",
      "Epoch [11/1000], spearman: 0.0052, Loss: 204.9633\n",
      "Epoch [12/1000], spearman: -0.0006, Loss: 204.9644\n",
      "Epoch [13/1000], spearman: 0.0013, Loss: 204.9630\n",
      "Epoch [14/1000], spearman: 0.0052, Loss: 204.9640\n",
      "Epoch [15/1000], spearman: -0.0050, Loss: 204.9639\n",
      "Epoch [16/1000], spearman: -0.0028, Loss: 204.9638\n",
      "Epoch [17/1000], spearman: -0.0012, Loss: 204.9632\n",
      "Epoch [18/1000], spearman: -0.0039, Loss: 204.9632\n",
      "Epoch [19/1000], spearman: -0.0019, Loss: 204.9634\n",
      "Epoch [20/1000], spearman: -0.0002, Loss: 204.9637\n",
      "Epoch [21/1000], spearman: 0.0006, Loss: 204.9632\n",
      "Epoch [22/1000], spearman: -0.0012, Loss: 204.9637\n",
      "Epoch [23/1000], spearman: 0.0002, Loss: 204.9623\n",
      "Epoch [24/1000], spearman: 0.0011, Loss: 204.9639\n",
      "Epoch [25/1000], spearman: -0.0051, Loss: 204.9641\n",
      "Epoch [26/1000], spearman: -0.0002, Loss: 204.9632\n",
      "Epoch [27/1000], spearman: 0.0038, Loss: 204.9629\n",
      "Epoch [28/1000], spearman: 0.0025, Loss: 204.9635\n",
      "Epoch [29/1000], spearman: -0.0025, Loss: 204.9638\n",
      "Epoch [30/1000], spearman: -0.0006, Loss: 204.9633\n",
      "Epoch [31/1000], spearman: 0.0025, Loss: 204.9630\n",
      "Epoch [32/1000], spearman: 0.0039, Loss: 204.9628\n",
      "Epoch [33/1000], spearman: 0.0041, Loss: 204.9627\n",
      "Epoch [34/1000], spearman: 0.0002, Loss: 204.9638\n",
      "Epoch [35/1000], spearman: -0.0023, Loss: 204.9634\n",
      "Epoch [36/1000], spearman: -0.0052, Loss: 204.9642\n",
      "Epoch [37/1000], spearman: -0.0004, Loss: 204.9631\n",
      "Epoch [38/1000], spearman: -0.0048, Loss: 204.9635\n",
      "Epoch [39/1000], spearman: -0.0004, Loss: 204.9635\n",
      "Epoch [40/1000], spearman: -0.0050, Loss: 204.9635\n",
      "Epoch [41/1000], spearman: -0.0087, Loss: 204.9635\n",
      "Epoch [42/1000], spearman: -0.0033, Loss: 204.9631\n",
      "Epoch [43/1000], spearman: 0.0041, Loss: 204.9631\n",
      "Epoch [44/1000], spearman: 0.0025, Loss: 204.9629\n",
      "Epoch [45/1000], spearman: 0.0065, Loss: 204.9628\n",
      "Epoch [46/1000], spearman: 0.0058, Loss: 204.9623\n",
      "Epoch [47/1000], spearman: -0.0039, Loss: 204.9633\n",
      "Epoch [48/1000], spearman: 0.0069, Loss: 204.9617\n",
      "Epoch [49/1000], spearman: 0.0033, Loss: 204.9636\n",
      "Epoch [50/1000], spearman: -0.0035, Loss: 204.9634\n",
      "Epoch [51/1000], spearman: -0.0020, Loss: 204.9644\n",
      "Epoch [52/1000], spearman: -0.0010, Loss: 204.9622\n",
      "Epoch [53/1000], spearman: -0.0024, Loss: 204.9638\n",
      "Epoch [54/1000], spearman: -0.0007, Loss: 204.9629\n",
      "Epoch [55/1000], spearman: -0.0022, Loss: 204.9636\n",
      "Epoch [56/1000], spearman: 0.0038, Loss: 204.9636\n",
      "Epoch [57/1000], spearman: -0.0010, Loss: 204.9631\n",
      "Epoch [58/1000], spearman: -0.0007, Loss: 204.9632\n",
      "Epoch [59/1000], spearman: -0.0004, Loss: 204.9636\n",
      "Epoch [60/1000], spearman: -0.0048, Loss: 204.9635\n",
      "Epoch [61/1000], spearman: 0.0021, Loss: 204.9632\n",
      "Epoch [62/1000], spearman: -0.0077, Loss: 204.9642\n",
      "Epoch [63/1000], spearman: 0.0046, Loss: 204.9632\n",
      "Epoch [64/1000], spearman: 0.0006, Loss: 204.9632\n",
      "Epoch [65/1000], spearman: 0.0089, Loss: 204.9627\n",
      "Epoch [66/1000], spearman: -0.0023, Loss: 204.9632\n",
      "Epoch [67/1000], spearman: 0.0096, Loss: 204.9624\n",
      "Epoch [68/1000], spearman: 0.0022, Loss: 204.9629\n",
      "Epoch [69/1000], spearman: -0.0013, Loss: 204.9636\n",
      "Epoch [70/1000], spearman: 0.0058, Loss: 204.9625\n",
      "Epoch [71/1000], spearman: -0.0015, Loss: 204.9634\n",
      "Epoch [72/1000], spearman: -0.0012, Loss: 204.9642\n",
      "Epoch [73/1000], spearman: 0.0054, Loss: 204.9625\n",
      "Epoch [74/1000], spearman: 0.0032, Loss: 204.9631\n",
      "Epoch [75/1000], spearman: -0.0054, Loss: 204.9633\n",
      "Epoch [76/1000], spearman: -0.0044, Loss: 204.9632\n",
      "Epoch [77/1000], spearman: -0.0003, Loss: 204.9637\n",
      "Epoch [78/1000], spearman: -0.0005, Loss: 204.9633\n",
      "Epoch [79/1000], spearman: 0.0031, Loss: 204.9629\n",
      "Epoch [80/1000], spearman: 0.0068, Loss: 204.9633\n",
      "Epoch [81/1000], spearman: -0.0032, Loss: 204.9631\n",
      "Epoch [82/1000], spearman: -0.0026, Loss: 204.9631\n",
      "Epoch [83/1000], spearman: -0.0091, Loss: 204.9639\n",
      "Epoch [84/1000], spearman: 0.0015, Loss: 204.9632\n",
      "Epoch [85/1000], spearman: 0.0012, Loss: 204.9631\n",
      "Epoch [86/1000], spearman: 0.0026, Loss: 204.9631\n",
      "Epoch [87/1000], spearman: 0.0035, Loss: 204.9629\n",
      "Epoch [88/1000], spearman: 0.0022, Loss: 204.9631\n",
      "Epoch [89/1000], spearman: -0.0078, Loss: 204.9636\n",
      "Epoch [90/1000], spearman: 0.0013, Loss: 204.9626\n",
      "Epoch [91/1000], spearman: -0.0000, Loss: 204.9634\n",
      "Epoch [92/1000], spearman: 0.0006, Loss: 204.9631\n",
      "Epoch [93/1000], spearman: -0.0053, Loss: 204.9632\n",
      "Epoch [94/1000], spearman: -0.0006, Loss: 204.9633\n",
      "Epoch [95/1000], spearman: 0.0024, Loss: 204.9629\n",
      "Epoch [96/1000], spearman: 0.0019, Loss: 204.9628\n",
      "Epoch [97/1000], spearman: -0.0081, Loss: 204.9637\n",
      "Epoch [98/1000], spearman: 0.0010, Loss: 204.9625\n",
      "Epoch [99/1000], spearman: 0.0026, Loss: 204.9630\n",
      "Epoch [100/1000], spearman: 0.0065, Loss: 204.9623\n",
      "Epoch [101/1000], spearman: -0.0078, Loss: 204.9633\n",
      "Epoch [102/1000], spearman: -0.0024, Loss: 204.9634\n",
      "Epoch [103/1000], spearman: -0.0018, Loss: 204.9635\n",
      "Epoch [104/1000], spearman: 0.0032, Loss: 204.9627\n",
      "Epoch [105/1000], spearman: 0.0045, Loss: 204.9630\n",
      "Epoch [106/1000], spearman: 0.0022, Loss: 204.9628\n",
      "Epoch [107/1000], spearman: 0.0021, Loss: 204.9633\n",
      "Epoch [108/1000], spearman: -0.0011, Loss: 204.9632\n",
      "Epoch [109/1000], spearman: 0.0040, Loss: 204.9629\n",
      "Epoch [110/1000], spearman: 0.0004, Loss: 204.9636\n",
      "Epoch [111/1000], spearman: -0.0095, Loss: 204.9636\n",
      "Epoch [112/1000], spearman: 0.0034, Loss: 204.9626\n",
      "Epoch [113/1000], spearman: 0.0003, Loss: 204.9637\n",
      "Epoch [114/1000], spearman: -0.0040, Loss: 204.9636\n",
      "Epoch [115/1000], spearman: 0.0070, Loss: 204.9635\n",
      "Epoch [116/1000], spearman: -0.0033, Loss: 204.9634\n",
      "Epoch [117/1000], spearman: -0.0040, Loss: 204.9633\n",
      "Epoch [118/1000], spearman: 0.0039, Loss: 204.9628\n",
      "Epoch [119/1000], spearman: -0.0004, Loss: 204.9632\n",
      "Epoch [120/1000], spearman: 0.0017, Loss: 204.9632\n",
      "Epoch [121/1000], spearman: -0.0012, Loss: 204.9632\n",
      "Epoch [122/1000], spearman: -0.0007, Loss: 204.9632\n",
      "Epoch [123/1000], spearman: -0.0005, Loss: 204.9628\n",
      "Epoch [124/1000], spearman: -0.0070, Loss: 204.9633\n",
      "Epoch [125/1000], spearman: 0.0115, Loss: 204.9626\n",
      "Epoch [126/1000], spearman: 0.0097, Loss: 204.9608\n",
      "Epoch [127/1000], spearman: 0.0054, Loss: 204.9629\n",
      "Epoch [128/1000], spearman: 0.0019, Loss: 204.9630\n",
      "Epoch [129/1000], spearman: -0.0073, Loss: 204.9637\n",
      "Epoch [130/1000], spearman: -0.0015, Loss: 204.9641\n",
      "Epoch [131/1000], spearman: -0.0034, Loss: 204.9630\n",
      "Epoch [132/1000], spearman: -0.0009, Loss: 204.9631\n",
      "Epoch [133/1000], spearman: 0.0003, Loss: 204.9633\n",
      "Epoch [134/1000], spearman: -0.0070, Loss: 204.9643\n",
      "Epoch [135/1000], spearman: -0.0021, Loss: 204.9631\n",
      "Epoch [136/1000], spearman: 0.0045, Loss: 204.9627\n",
      "Epoch [137/1000], spearman: 0.0064, Loss: 204.9629\n",
      "Epoch [138/1000], spearman: 0.0040, Loss: 204.9628\n",
      "Epoch [139/1000], spearman: -0.0029, Loss: 204.9632\n",
      "Epoch [140/1000], spearman: 0.0064, Loss: 204.9627\n",
      "Epoch [141/1000], spearman: -0.0015, Loss: 204.9627\n",
      "Epoch [142/1000], spearman: -0.0053, Loss: 204.9632\n",
      "Epoch [143/1000], spearman: 0.0007, Loss: 204.9637\n",
      "Epoch [144/1000], spearman: -0.0007, Loss: 204.9629\n",
      "Epoch [145/1000], spearman: 0.0037, Loss: 204.9631\n",
      "Epoch [146/1000], spearman: 0.0018, Loss: 204.9633\n",
      "Epoch [147/1000], spearman: 0.0116, Loss: 204.9629\n",
      "Epoch [148/1000], spearman: -0.0006, Loss: 204.9629\n",
      "Epoch [149/1000], spearman: 0.0038, Loss: 204.9624\n",
      "Epoch [150/1000], spearman: 0.0013, Loss: 204.9627\n",
      "Epoch [151/1000], spearman: 0.0018, Loss: 204.9628\n",
      "Epoch [152/1000], spearman: -0.0042, Loss: 204.9641\n",
      "Epoch [153/1000], spearman: -0.0045, Loss: 204.9634\n",
      "Epoch [154/1000], spearman: -0.0019, Loss: 204.9638\n",
      "Epoch [155/1000], spearman: -0.0015, Loss: 204.9631\n",
      "Epoch [156/1000], spearman: 0.0036, Loss: 204.9629\n",
      "Epoch [157/1000], spearman: 0.0000, Loss: 204.9628\n",
      "Epoch [158/1000], spearman: -0.0014, Loss: 204.9630\n",
      "Epoch [159/1000], spearman: -0.0032, Loss: 204.9638\n",
      "Epoch [160/1000], spearman: -0.0047, Loss: 204.9635\n",
      "Epoch [161/1000], spearman: -0.0035, Loss: 204.9635\n",
      "Epoch [162/1000], spearman: 0.0060, Loss: 204.9629\n",
      "Epoch [163/1000], spearman: -0.0105, Loss: 204.9636\n",
      "Epoch [164/1000], spearman: -0.0005, Loss: 204.9628\n",
      "Epoch [165/1000], spearman: 0.0041, Loss: 204.9630\n",
      "Epoch [166/1000], spearman: -0.0037, Loss: 204.9634\n",
      "Epoch [167/1000], spearman: -0.0065, Loss: 204.9637\n",
      "Epoch [168/1000], spearman: -0.0009, Loss: 204.9631\n",
      "Epoch [169/1000], spearman: -0.0036, Loss: 204.9633\n",
      "Epoch [170/1000], spearman: 0.0042, Loss: 204.9627\n",
      "Epoch [171/1000], spearman: 0.0010, Loss: 204.9631\n",
      "Epoch [172/1000], spearman: 0.0019, Loss: 204.9631\n",
      "Epoch [173/1000], spearman: -0.0012, Loss: 204.9631\n",
      "Epoch [174/1000], spearman: -0.0037, Loss: 204.9634\n",
      "Epoch [175/1000], spearman: 0.0009, Loss: 204.9630\n",
      "Epoch [176/1000], spearman: -0.0025, Loss: 204.9632\n",
      "Epoch [177/1000], spearman: -0.0041, Loss: 204.9632\n",
      "Epoch [178/1000], spearman: 0.0010, Loss: 204.9630\n",
      "Epoch [179/1000], spearman: -0.0020, Loss: 204.9636\n",
      "Epoch [180/1000], spearman: -0.0013, Loss: 204.9634\n",
      "Epoch [181/1000], spearman: 0.0007, Loss: 204.9633\n",
      "Epoch [182/1000], spearman: -0.0004, Loss: 204.9632\n",
      "Epoch [183/1000], spearman: 0.0010, Loss: 204.9630\n",
      "Epoch [184/1000], spearman: 0.0025, Loss: 204.9630\n",
      "Epoch [185/1000], spearman: -0.0022, Loss: 204.9630\n",
      "Epoch [186/1000], spearman: -0.0006, Loss: 204.9628\n",
      "Epoch [187/1000], spearman: -0.0019, Loss: 204.9629\n",
      "Epoch [188/1000], spearman: 0.0037, Loss: 204.9628\n",
      "Epoch [189/1000], spearman: 0.0029, Loss: 204.9636\n",
      "Epoch [190/1000], spearman: -0.0008, Loss: 204.9632\n",
      "Epoch [191/1000], spearman: 0.0005, Loss: 204.9632\n",
      "Epoch [192/1000], spearman: 0.0036, Loss: 204.9631\n",
      "Epoch [193/1000], spearman: -0.0020, Loss: 204.9634\n",
      "Epoch [194/1000], spearman: -0.0014, Loss: 204.9630\n",
      "Epoch [195/1000], spearman: 0.0011, Loss: 204.9632\n",
      "Epoch [196/1000], spearman: 0.0002, Loss: 204.9632\n",
      "Epoch [197/1000], spearman: -0.0047, Loss: 204.9629\n",
      "Epoch [198/1000], spearman: 0.0017, Loss: 204.9631\n",
      "Epoch [199/1000], spearman: -0.0005, Loss: 204.9635\n",
      "Epoch [200/1000], spearman: 0.0010, Loss: 204.9627\n",
      "Epoch [201/1000], spearman: -0.0007, Loss: 204.9630\n",
      "Epoch [202/1000], spearman: 0.0025, Loss: 204.9633\n",
      "Epoch [203/1000], spearman: 0.0002, Loss: 204.9634\n",
      "Epoch [204/1000], spearman: -0.0005, Loss: 204.9630\n",
      "Epoch [205/1000], spearman: 0.0001, Loss: 204.9633\n",
      "Epoch [206/1000], spearman: 0.0010, Loss: 204.9633\n",
      "Epoch [207/1000], spearman: 0.0009, Loss: 204.9628\n",
      "Epoch [208/1000], spearman: 0.0001, Loss: 204.9629\n",
      "Epoch [209/1000], spearman: -0.0035, Loss: 204.9633\n",
      "Epoch [210/1000], spearman: 0.0005, Loss: 204.9629\n",
      "Epoch [211/1000], spearman: -0.0018, Loss: 204.9630\n",
      "Epoch [212/1000], spearman: -0.0009, Loss: 204.9630\n",
      "Epoch [213/1000], spearman: -0.0021, Loss: 204.9637\n",
      "Epoch [214/1000], spearman: 0.0032, Loss: 204.9629\n",
      "Epoch [215/1000], spearman: 0.0028, Loss: 204.9626\n",
      "Epoch [216/1000], spearman: 0.0042, Loss: 204.9632\n",
      "Epoch [217/1000], spearman: -0.0021, Loss: 204.9634\n",
      "Epoch [218/1000], spearman: -0.0024, Loss: 204.9632\n",
      "Epoch [219/1000], spearman: 0.0018, Loss: 204.9632\n",
      "Epoch [220/1000], spearman: -0.0010, Loss: 204.9632\n",
      "Epoch [221/1000], spearman: -0.0034, Loss: 204.9634\n",
      "Epoch [222/1000], spearman: 0.0001, Loss: 204.9635\n",
      "Epoch [223/1000], spearman: 0.0007, Loss: 204.9629\n",
      "Epoch [224/1000], spearman: -0.0003, Loss: 204.9633\n",
      "Epoch [225/1000], spearman: 0.0001, Loss: 204.9630\n",
      "Epoch [226/1000], spearman: -0.0016, Loss: 204.9633\n",
      "Epoch [227/1000], spearman: -0.0007, Loss: 204.9634\n",
      "Epoch [228/1000], spearman: -0.0008, Loss: 204.9629\n",
      "Epoch [229/1000], spearman: 0.0021, Loss: 204.9632\n",
      "Epoch [230/1000], spearman: 0.0001, Loss: 204.9631\n",
      "Epoch [231/1000], spearman: 0.0002, Loss: 204.9630\n",
      "Epoch [232/1000], spearman: 0.0010, Loss: 204.9631\n",
      "Epoch [233/1000], spearman: 0.0011, Loss: 204.9631\n",
      "Epoch [234/1000], spearman: -0.0005, Loss: 204.9630\n",
      "Epoch [235/1000], spearman: -0.0011, Loss: 204.9631\n",
      "Epoch [236/1000], spearman: 0.0011, Loss: 204.9632\n",
      "Epoch [237/1000], spearman: 0.0002, Loss: 204.9634\n",
      "Epoch [238/1000], spearman: 0.0000, Loss: 204.9632\n",
      "Epoch [239/1000], spearman: -0.0006, Loss: 204.9632\n",
      "Epoch [240/1000], spearman: 0.0013, Loss: 204.9629\n",
      "Epoch [241/1000], spearman: 0.0001, Loss: 204.9632\n",
      "Epoch [242/1000], spearman: 0.0005, Loss: 204.9629\n",
      "Epoch [243/1000], spearman: -0.0007, Loss: 204.9635\n",
      "Epoch [244/1000], spearman: 0.0002, Loss: 204.9630\n",
      "Epoch [245/1000], spearman: 0.0003, Loss: 204.9632\n",
      "Epoch [246/1000], spearman: 0.0001, Loss: 204.9632\n",
      "Epoch [247/1000], spearman: 0.0004, Loss: 204.9631\n",
      "Epoch [248/1000], spearman: -0.0016, Loss: 204.9629\n",
      "Epoch [249/1000], spearman: -0.0017, Loss: 204.9631\n",
      "Epoch [250/1000], spearman: -0.0012, Loss: 204.9630\n",
      "Epoch [251/1000], spearman: 0.0009, Loss: 204.9632\n",
      "Epoch [252/1000], spearman: 0.0001, Loss: 204.9631\n",
      "Epoch [253/1000], spearman: 0.0013, Loss: 204.9630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m total_sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqurry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/esm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/esm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/esm/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/SimProteinData/scripts/data_utils.py:31\u001b[0m, in \u001b[0;36mtransfer_dataset.collate_fn\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_data):\n\u001b[0;32m---> 31\u001b[0m     qurry \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([u[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m batch_data]))\n\u001b[1;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([u[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m batch_data]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load dataset\n",
    "embed_path='../data/DMS_substitutionsesm_embed'\n",
    "labels_path='../data/dataset_cor_random.csv'\n",
    "dataset = transfer_dataset(embed_path, labels_path)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "hidden_dim = 512\n",
    "out_dim = 128\n",
    "model = LayerNormNet(hidden_dim, out_dim, device=device, dtype=torch.float).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "log_dir = './runs'\n",
    "writer = SummaryWriter(log_dir)\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_sr = 0\n",
    "    for batch in dataloader:\n",
    "        qurry, data, label = [x.to(device) for x in batch]\n",
    "        \n",
    "        # Forward pass\n",
    "        output1 = model(qurry)\n",
    "        output2 = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        sr,loss = criterion(output1, output2, label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_sr+=sr\n",
    "    writer.add_scalar('Train/Loss', total_loss/len(dataloader), epoch)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], spearman: {total_sr/len(dataloader):.4f}, Loss: {total_loss/len(dataloader):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
